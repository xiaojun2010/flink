version: '3.7'

services:

# ## kafka docker
  ## 1台 broker

  kafka1:
    image: wurstmeister/kafka:2.13-2.8.1
    container_name: kafka1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      TZ: Asia/Shanghai
      KAFKA_LISTENERS: PLAINTEXT://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_ADVERTISED_HOST_NAME: kafka1
      KAFKA_LOG_RETENTION_HOURS: 24
      KAFKA_CREATE_TOPICS: "imoocevent:1:1,imooctest:1:1"
    # volumes:
    #   - ./config/prometheus/node_exporter:/node_exporter
    # command: /node_exporter

    
## redis集群
## 6台机集群·

  redis:
    image: grokzen/redis-cluster:6.2.1
    container_name: redis
    ports:
      - "7000:7000"
      - "7001:7001"
      - "7002:7002"
      - "7003:7003"
      - "7004:7004"
      - "7005:7005"
      - "9104:9100"
    environment:
      IP: 0.0.0.0
      TZ: Asia/Shanghai
    volumes:
      # - ./config/prometheus/node_exporter:/node_exporter
      - ./volumes/redis:/redis-data
    # command: /node_exporter
    
# ## flink
  ## 1台 jobmanager , 3台 taskmanager

  jobmanager:
    image: flink:1.14.5-scala_2.11-java8
    container_name: jobmanager
    expose:
      - "6123"
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: jobmanager
      TZ: Asia/Shanghai
      HADOOP_CONF_DIR: /hadoop/etc
      YARN_CONF_DIR: /hadoop/etc
    deploy:
      resources:
        limits:
          memory: 2G
    volumes:
      - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
      # - ./config/flink/log4j.properties:/opt/flink/conf/log4j.properties
      - ./config/hadoop/core-site.xml:/hadoop/etc/core-site.xml
      - ./config/hadoop/yarn-site.xml:/hadoop/etc/yarn-site.xml
      - ./config/hadoop/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar
    
  
  taskmanager-1:
    image: flink:1.14.5-scala_2.11-java8
    container_name: taskmanager-1
    depends_on:
      - jobmanager
    expose:
      - "6121"
      - "6122"
    command: taskmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: jobmanager
      TZ: Asia/Shanghai
      HADOOP_CONF_DIR: /hadoop/etc
      YARN_CONF_DIR: /hadoop/etc
    deploy:
      resources:
        limits:
          memory: 3G
    volumes:
      - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
      # - ./config/flink/log4j.properties:/opt/flink/conf/log4j.properties
      - ./config/hadoop/core-site.xml:/hadoop/etc/core-site.xml
      - ./config/hadoop/yarn-site.xml:/hadoop/etc/yarn-site.xml
      - ./config/hadoop/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar
    

  taskmanager-2:
    image: flink:1.14.5-scala_2.11-java8
    container_name: taskmanager-2
    depends_on:
      - jobmanager
    expose:
      - "6121"
      - "6122"
    command: taskmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: jobmanager
      TZ: Asia/Shanghai
      HADOOP_CONF_DIR: /hadoop/etc
      YARN_CONF_DIR: /hadoop/etc
    deploy:
      resources:
        limits:
          memory: 3G
    volumes:
      - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
      # - ./config/flink/log4j.properties:/opt/flink/conf/log4j.properties
      - ./config/hadoop/core-site.xml:/hadoop/etc/core-site.xml
      - ./config/hadoop/yarn-site.xml:/hadoop/etc/yarn-site.xml
      - ./config/hadoop/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar
    


  taskmanager-3:
    image: flink:1.14.5-scala_2.11-java8
    container_name: taskmanager-3
    depends_on:
      - jobmanager
    expose:
      - "6121"
      - "6122"
    command: taskmanager
    environment:
      JOB_MANAGER_RPC_ADDRESS: jobmanager
      TZ: Asia/Shanghai
      HADOOP_CONF_DIR: /hadoop/etc
      YARN_CONF_DIR: /hadoop/etc
    deploy:
      resources:
        limits:
          memory: 3G
    volumes:
      - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
      # - ./config/flink/log4j.properties:/opt/flink/conf/log4j.properties
      - ./config/hadoop/core-site.xml:/hadoop/etc/core-site.xml
      - ./config/hadoop/yarn-site.xml:/hadoop/etc/yarn-site.xml
      - ./config/hadoop/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar
    


# ## 普罗米修斯监控

  prometheus:
    image: prom/prometheus:v2.38.0
    container_name: prometheus
    environment:
      TZ: Asia/Shanghai
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'

  grafana:
    image: grafana/grafana:9.0.9
    container_name: grafana
    depends_on:
      - prometheus
    environment:
      TZ: Asia/Shanghai
    ports:
      - "3000:3000"
    volumes:
      - ./volumes/prometheus/grafana-data:/var/lib/grafana
      - ./config/prometheus/grafana/defaults.ini:/etc/grafana/grafana.ini
      - ./config/prometheus/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/prometheus/grafana/datasources:/etc/grafana/provisioning/datasources

  flume_exporter:
    image: zhijunwoo/flume_exporter:latest
    container_name: flume_exporter
    depends_on:
      - flume
    environment:
      TZ: Asia/Shanghai
    ports:
      - "9360:9360"
    volumes:
      - ./config/prometheus/flume_exporter/config.yml:/etc/flume_exporter/config.yml
      - ./config/prometheus/flume_exporter/metrics.yml:/etc/flume_exporter/metrics.yml

  kafka_exporter:
    image: danielqsj/kafka-exporter:v1.6.0
    container_name: kafka_exporter
    depends_on:
      - kafka1
    environment:
      TZ: Asia/Shanghai
    ports:
      - "9308:9308"
    command:
      - '--kafka.server=kafka1:9092'
      - '--kafka.version=2.8.1'
      - '--log.level=info'
    
  

# ## 其他 docker

  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    container_name: zookeeper
    environment:
      TZ: Asia/Shanghai
    deploy:
      resources:
        limits:
          memory: 2G
    ports:
      - "2181:2181"


  # ## flume

  flume:
    image: probablyfine/flume:2.0.0
    container_name: flume
    environment:
      FLUME_AGENT_NAME: imooc
      TZ: Asia/Shanghai
    ports:
      - "36001:36001"
    deploy:
      resources:
        limits:
          memory: 1G
    volumes:
      - ./config/flume/flume-env.sh:/opt/flume-config/flume.conf/flume-env.sh
      - ./config/flume/log4j.properties:/opt/flume/conf/log4j.properties
      - ./config/flume/conf:/imooc-flume-conf
      - ./volumes/flume/dataset:/imooc-flume
      - ./volumes/flume/imooc-position:/imooc-position
      - ./volumes/flume/imooc-chn-chk:/imooc-flume-chn-chk
      - ./volumes/flume/imooc-chn-data:/imooc-flume-chn-data
      - ./volumes/flume/logs:/logs


  hbase:
    build: ./config/hbase
    container_name: hbase
    hostname: hbase
    depends_on:
      - hadoop
      - zookeeper
    environment:
      TZ: Asia/Shanghai
    ports:
      - "8765:8765"
      - "60000:60000"
      - "60010:60010"
      - "60020:60020"
      - "60030:60030"

  hadoop:
    image: harisekhon/hadoop:2.7
    container_name: hadoop
    hostname: hadoop
    environment:
      TZ: Asia/Shanghai
    ports:
      - "8042:8042"
      - "8088:8088"
      - "19888:19888"
      - "50070:50070"
      - "50075:50075"
      - "8020:8020"
      - "50090:50090"
    volumes:
      - ./volumes/dfs/name:/dfs/name:rw
      - ./volumes/dfs/data:/dfs/data:rw
      - ./volumes/dfs/tmp:/dfs/tmp:rw
      - ./config/hadoop/yarn-site.xml:/hadoop/etc/hadoop/yarn-site.xml
      - ./config/hadoop/hdfs-site.xml:/hadoop/etc/hadoop/hdfs-site.xml
      - ./config/hadoop/core-site.xml:/hadoop/etc/hadoop/core-site.xml
      - ./config/hadoop/entrypoint.sh:/entrypoint.sh
      

  clickhouse:
    image: yandex/clickhouse-server:21.1.9.41
    container_name: clickhouse
    environment:
      TZ: Asia/Shanghai
    ports:
      - "8123:8123"
      - "9000:9000"
    deploy:
      resources:
        limits:
          memory: 4G
    volumes:
      - ./volumes/clickhouse:/var/lib/clickhouse


  mysql:
    image: mysql:5.7.25
    container_name: mysql
    environment:
      MYSQL_ROOT_PASSWORD: "123456"
      TZ: "Asia/Shanghai"
    ports:
      - "3306:3306"
    deploy:
      resources:
        limits:
          memory: 1G
    volumes:
      - ./config/mysql/mysqld.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf
      - ./volumes/mysql/data:/var/lib/mysql
    
# ## 定时任务
  crontab:
    build: ./config/crontab
    container_name: crontab
    deploy:
      resources:
        limits:
          memory: 100M
    volumes:
      - ./jar/dataSet:/data
      - ./jar/crontab:/sh
      - ./volumes/flume/dataset:/flume
      - ./volumes/crontab/logs:/logs
      

# ## 前端
  nodejs:
    image: node:20.5.1
    container_name: nodejs
    environment:
      TZ: Asia/Shanghai
    ports:
      - "6080:6080"
    deploy:
      resources:
        limits:
          memory: 100M
    working_dir: /home/node
    command: "npm run start"
    volumes:
      - ./config/nodejs/node_modules:/home/node/node_modules
      - ./config/nodejs/index.html:/home/node/index.html
      - ./config/nodejs/pages:/home/node/pages
      - ./config/nodejs/amis:/home/node/amis
      - ./config/nodejs/history:/home/node/history
      - ./config/nodejs/package.json:/home/node/package.json


  